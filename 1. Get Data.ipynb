{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de69e8a4",
   "metadata": {},
   "source": [
    "# Data Download & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2818c",
   "metadata": {},
   "source": [
    "The goal of this notebook is to download, filter, and clean news & bar data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27de09",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0b7dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import ast\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "from alpaca_api import AlpacaRequester\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3a67e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ".env structure:\n",
    "\n",
    "APCA_API_KEY_ID=<your_alpaca_api_key_id>\n",
    "APCA_API_SECRET_KEY=<your_alpaca_api_secret_key>\n",
    "\"\"\"\n",
    "\n",
    "assert load_dotenv()  # by default loads from .env in current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f1c4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "START = str(config['START'])\n",
    "END = str(config['END'])\n",
    "PREDICTION_DELTA = config['PREDICTION_DELTA']\n",
    "OPEN_CUTOFF = config['OPEN_CUTOFF']\n",
    "CLOSE_CUTOFF = config['CLOSE_CUTOFF']\n",
    "FWD_WINDOW = config['FWD_WINDOW']\n",
    "BACK_WINDOW = config['BACK_WINDOW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "063fc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = AlpacaRequester()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e4906",
   "metadata": {},
   "source": [
    "# 1. Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8abc6f",
   "metadata": {},
   "source": [
    "## 1.1. Getting News Article Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2f86c",
   "metadata": {},
   "source": [
    "We'll download all news articles from January 1, 2020 to the present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b70f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take 2hrs + to run\n",
    "GET_NEWS = False\n",
    "if GET_NEWS:\n",
    "    AR.get_news(start=START,exclude_contentless=False,include_content=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c395d",
   "metadata": {},
   "source": [
    "Next we'll filter to the symbols we care about, and clean them up. The ignored symbols are ignored because they're:\n",
    "- not American (we are using American news articles, so best to stick with American stocks)\n",
    "- too volatile\n",
    "- too small\n",
    "- meme stocks\n",
    "- crypto or crypto-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38764927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/f9wf3kz948538_qhqyrxj0xc0000gn/T/ipykernel_24667/626317480.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  articles_df: pd.DataFrame = pd.read_csv(\"news/news.csv\")\n",
      "100%|██████████| 1278601/1278601 [00:08<00:00, 142597.18it/s]\n"
     ]
    }
   ],
   "source": [
    "CRYPTO_SYMBOLS = ['BTC', 'BTCUSD', 'ETHUSD', 'DOGEUSD', 'SHIBUSD', 'SOLUSD','BTC','$BTC']\n",
    "ETF_SYMBOLS = ['EWU','FXI','GLD','QQQ','RSX','SPY','USO','VGK','UNG']\n",
    "BAD_SYMBOLS = ['BBBY','TWTR','ABC','ACB','CGC','TLRY','AMC','GME','NKLA','CVNA','LCID','NVAX','MARA','RIOT','MSTR','PLUG','PTON','SNAP','SPCE','COIN','HOOD','XPEV','NIO','LI','RIVN','RBLX','DIA','ATVI','LYFT','CCL','BABA','BIDU','JD','PDD','SONY','STLA','AZN','NVO','TCEHY','SNDL','TM','GSK','AFRM','DKNG','PENN','BNTX','BBY','BYND','SOFI','SNY']\n",
    "REPLACE_SYMBOLS = {'FB': 'META','GOOG':'GOOGL'}\n",
    "def format_symbols(symbols):\n",
    "    symbols = ast.literal_eval(symbols)\n",
    "    symbols = [(i if i not in REPLACE_SYMBOLS else REPLACE_SYMBOLS[i]) for i in symbols if i not in CRYPTO_SYMBOLS + ETF_SYMBOLS + BAD_SYMBOLS]\n",
    "    symbols = set(map(lambda x:x.replace('$','').replace('-','.'),symbols))\n",
    "    return list(symbols)\n",
    "\n",
    "articles_df: pd.DataFrame = pd.read_csv(\"news/news.csv\")\n",
    "articles_df['symbols'] = articles_df['symbols'].progress_apply(format_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69b48137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAL',\n",
       " 'AAPL',\n",
       " 'ABBV',\n",
       " 'ABNB',\n",
       " 'ABT',\n",
       " 'ACN',\n",
       " 'ADBE',\n",
       " 'AMAT',\n",
       " 'AMD',\n",
       " 'AMGN',\n",
       " 'AMZN',\n",
       " 'AVGO',\n",
       " 'AXP',\n",
       " 'BA',\n",
       " 'BAC',\n",
       " 'BB',\n",
       " 'BIIB',\n",
       " 'BLK',\n",
       " 'BMY',\n",
       " 'BX',\n",
       " 'C',\n",
       " 'CHWY',\n",
       " 'CMCSA',\n",
       " 'CMG',\n",
       " 'COST',\n",
       " 'CRM',\n",
       " 'CRWD',\n",
       " 'CSCO',\n",
       " 'CVS',\n",
       " 'CVX',\n",
       " 'DAL',\n",
       " 'DASH',\n",
       " 'DELL',\n",
       " 'DIS',\n",
       " 'ENPH',\n",
       " 'F',\n",
       " 'FDX',\n",
       " 'GE',\n",
       " 'GILD',\n",
       " 'GM',\n",
       " 'GOOGL',\n",
       " 'GS',\n",
       " 'HD',\n",
       " 'IBM',\n",
       " 'INTC',\n",
       " 'JNJ',\n",
       " 'JPM',\n",
       " 'KO',\n",
       " 'LLY',\n",
       " 'LMT',\n",
       " 'LULU',\n",
       " 'LUV',\n",
       " 'MA',\n",
       " 'MCD',\n",
       " 'META',\n",
       " 'MGM',\n",
       " 'MRK',\n",
       " 'MRNA',\n",
       " 'MRVL',\n",
       " 'MS',\n",
       " 'MSFT',\n",
       " 'MU',\n",
       " 'NFLX',\n",
       " 'NKE',\n",
       " 'NOW',\n",
       " 'NVDA',\n",
       " 'ORCL',\n",
       " 'OXY',\n",
       " 'PANW',\n",
       " 'PFE',\n",
       " 'PINS',\n",
       " 'PLTR',\n",
       " 'PYPL',\n",
       " 'QCOM',\n",
       " 'RCL',\n",
       " 'REGN',\n",
       " 'ROKU',\n",
       " 'SBUX',\n",
       " 'SHOP',\n",
       " 'SMCI',\n",
       " 'SNOW',\n",
       " 'SPOT',\n",
       " 'SQ',\n",
       " 'T',\n",
       " 'TGT',\n",
       " 'TMUS',\n",
       " 'TSLA',\n",
       " 'TSM',\n",
       " 'UAL',\n",
       " 'UBER',\n",
       " 'UNH',\n",
       " 'UPS',\n",
       " 'V',\n",
       " 'VZ',\n",
       " 'WBD',\n",
       " 'WFC',\n",
       " 'WMT',\n",
       " 'XOM',\n",
       " 'ZM',\n",
       " 'ZS'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_counter = Counter([i for sublist in articles_df['symbols'] for i in sublist])\n",
    "symbols, _ = zip(*symbol_counter.most_common(100))\n",
    "symbols = set(symbols)\n",
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73d6abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As of mid July 2025, this will download about 60GB+ of data\n",
    "GET_BARS = False\n",
    "if GET_BARS:\n",
    "    AR.get_bars(store_url=False,store_token=False,symbols=symbols,start=START,timeframe=\"1T\",adjustment=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bef70",
   "metadata": {},
   "source": [
    "# 2. Data cleaning & filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7458d",
   "metadata": {},
   "source": [
    "## 2.1. Clean & store bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44736eba",
   "metadata": {},
   "source": [
    "We will restrict to only those bars that:\n",
    "- Occur during normal trading hours, according to the calendar\n",
    "- Occur on days where that stock has at least 60 1-minute trading bars during normal trading hours\n",
    "\n",
    "The reason for this is because there are some mistakes in the Alpaca dataset, for example, some days only have one bar for each ticker.\n",
    "\n",
    "We also specifically get rid of any bars for a ticker for 2022-01-24 if there are 75 or less bars for that day, as there seems to be an error with that day where for some tickers only the first 75 minutes of bars are available.\n",
    "\n",
    "Lastly, we get rid of any IPO days, as they are much too volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a08891",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = AR.market_calendar(start=START,end=END)[['date','open','close']]\n",
    "calendar[\"open\"] = pd.to_datetime(\n",
    "    calendar[\"date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + calendar[\"open\"],\n",
    "    format=\"%Y-%m-%d %H:%M\",\n",
    ").dt.tz_localize(\"America/New_York\")\n",
    "calendar['close'] = pd.to_datetime(\n",
    "    calendar['date'].dt.strftime('%Y-%m-%d') + \" \" + calendar['close'],\n",
    "    format='%Y-%m-%d %H:%M'\n",
    ").dt.tz_localize('America/New_York')\n",
    "calendar.rename(columns={'open':'mkt_open','close':'mkt_close'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b0cee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:45<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Dictionary storing dates with bars after filtering for each ticker\n",
    "# These dates will be used to filter articles later\n",
    "dates = {}\n",
    "\n",
    "IPOs = {\n",
    "    'ABNB': pd.Timestamp(2020,12,10),\n",
    "    'DASH': pd.Timestamp(2020,12,9),\n",
    "    'PLTR': pd.Timestamp(2020,9,30),\n",
    "    'SNOW': pd.Timestamp(2020,9,16),\n",
    "}\n",
    "\n",
    "for symbol in tqdm(symbols,smoothing=0):\n",
    "    path = Path(f'bars/{symbol}.csv')\n",
    "    if path.exists() and path.is_file():\n",
    "        df = pd.read_csv(f'bars/{symbol}.csv')\n",
    "        df.drop_duplicates('t',keep='first',inplace=True)\n",
    "\n",
    "        # Format time and make date column\n",
    "        df['t'] = pd.to_datetime(df['t']).dt.tz_convert('America/New_York')\n",
    "        df['date'] = df['t'].dt.tz_convert(None).dt.normalize()\n",
    "\n",
    "        # Get mkt open and close times\n",
    "        df = df.merge(calendar[['date','mkt_open','mkt_close']],on='date',how='left')\n",
    "\n",
    "        # Filter to only bars that occur during the trading day\n",
    "        df = df[df['t'].between(df['mkt_open'],df['mkt_close'],inclusive='left')]\n",
    "\n",
    "        # Create dates that we'll keep; we want to get rid of:\n",
    "        # - IPO days\n",
    "        # - The aforementioned error day on 2022-01-24\n",
    "        # - days with < 60 bars\n",
    "        def filter_days(day_bars): \n",
    "            date = day_bars.name\n",
    "            c1 = not (symbol in IPOs and date == IPOs[symbol]) \n",
    "            c2 = (date != pd.Timestamp(2022,1,24) or len(day_bars) > 75) \n",
    "            c3 = (len(day_bars) >= 60)\n",
    "            return c1 and c2 and c3\n",
    "        dates[symbol] = set(df.groupby('date').filter(filter_days)['date'])\n",
    "\n",
    "        # Write to parquet file\n",
    "        df.to_parquet(f'bars/{symbol}.parquet',index=False)\n",
    "    else:\n",
    "        print(\"Missing bars for {tkr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69562fbf",
   "metadata": {},
   "source": [
    "## 2.2. Clean & store articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373b231",
   "metadata": {},
   "source": [
    "### 1. Select & format column types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ae3cd",
   "metadata": {},
   "source": [
    "First, we'll format the columns to the proper timezone and with the right types; this will make the dataframe take up less memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "492077d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at    object\n",
       "updated_at    object\n",
       "headline      object\n",
       "symbols       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get only cols we care about\n",
    "articles_df = articles_df[['created_at','updated_at','headline','symbols']]  # type: ignore\n",
    "articles_df: pd.DataFrame\n",
    "\n",
    "articles_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49dfd175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at    datetime64[ns, America/New_York]\n",
       "updated_at    datetime64[ns, America/New_York]\n",
       "headline                        string[python]\n",
       "symbols                                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df['headline'] = articles_df['headline'].astype(\"string\")\n",
    "articles_df['created_at'] = pd.to_datetime(articles_df['created_at']).dt.tz_convert(\"America/New_York\")\n",
    "articles_df['updated_at'] = pd.to_datetime(articles_df['updated_at']).dt.tz_convert(\"America/New_York\")\n",
    "articles_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a45975",
   "metadata": {},
   "source": [
    "### 2. Filter to universe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e1da9",
   "metadata": {},
   "source": [
    "We only want articles with tickers in our list of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1922c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = set(symbols)\n",
    "\n",
    "# Only keep articles that have at least one symbol in our list\n",
    "articles_df['symbols'] = articles_df['symbols'].apply(lambda x: [i for i in x if i in symbols])\n",
    "articles_df = articles_df[articles_df['symbols'].apply(len) > 0]  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69461e5d",
   "metadata": {},
   "source": [
    "### 3. Get rid of articles updated more than a minute after creation date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfe07b",
   "metadata": {},
   "source": [
    "We want to get rid of articles which were updated more than a minute after they were created to avoid lookahead bias. To start, we'll see how many articles we'd be getting rid of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2843a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.52% of articles have been updated after 60 seconds\n"
     ]
    }
   ],
   "source": [
    "mask = (articles_df['updated_at'] - articles_df['created_at']).dt.total_seconds() <= 60\n",
    "print(f\"{100 * len(articles_df[~mask]) / len(articles_df):.2f}% of articles have been updated after 60 seconds\")\n",
    "# Outputs: 5.52% of articles have been updated after 60 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092c65f",
   "metadata": {},
   "source": [
    "We're retaining 94.48% of articles, so this shouldn't impact coverage too badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2253baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6820342",
   "metadata": {},
   "source": [
    "### 4. Get rid of headlines which appear more than once and more than an hour apart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa641174",
   "metadata": {},
   "source": [
    "Next we'll get rid of any rows whose headline appears more than once with at least one occurrence being more than one hour apart. Any such headline is inherently not time-senstive, so we don't care about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4437740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = articles_df.groupby('headline')['created_at']\n",
    "mask = (g.transform('max') - g.transform('min')) <= pd.Timedelta('1h')\n",
    "articles_df = articles_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22c454",
   "metadata": {},
   "source": [
    "### 5. Keep first occurrence of each headline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed129fc",
   "metadata": {},
   "source": [
    "Now, we'll keep the first occurrence of each headline (if there are multiple headlines in a single hour presumably it is a duplicate article; we'll keep the first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20de1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df.sort_values('created_at').drop_duplicates(subset='headline', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ef018",
   "metadata": {},
   "source": [
    "### 6. Keep only articles which ocurred during trading hours for days we have bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18cf3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make date column\n",
    "articles_df['date'] = articles_df['created_at'].dt.tz_localize(None).dt.normalize()  # type: ignore\n",
    "\n",
    "# Make dataframe of trading dates by ticker\n",
    "date_df = pd.DataFrame([{'symbol': sym, 'date': d} for sym,ds in dates.items() for d in ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5146b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode to one symbol per row and filter to articles which occur on dates for which we have bars\n",
    "articles_df = articles_df.explode('symbols').rename(columns={'symbols':'symbol'}).merge(date_df, on=['symbol', 'date'])\n",
    "articles_df['symbol'] = articles_df['symbol'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7e8ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mkt open and close times\n",
    "articles_df = articles_df.merge(calendar,on='date',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a63cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to articles which occur during trading hours, and within the specified cutoff range\n",
    "mask = (\n",
    "    (~articles_df['mkt_open'].isna()) &\n",
    "    (~articles_df['mkt_close'].isna()) &\n",
    "    (articles_df['created_at'] >= articles_df['mkt_open'] + pd.Timedelta(minutes=OPEN_CUTOFF)) &\n",
    "    (articles_df['created_at'] < articles_df['mkt_close'] - pd.Timedelta(minutes=CLOSE_CUTOFF + 1))\n",
    ")\n",
    "articles_df = articles_df[mask].drop(columns=['date','mkt_open','mkt_close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41933994",
   "metadata": {},
   "source": [
    "### 7. Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6124ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.to_parquet('news/cleaned_news.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b58e0",
   "metadata": {},
   "source": [
    "## 2.3. Store symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4dc23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('symbols.pkl', 'wb') as f:\n",
    "    pickle.dump(symbols,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c1e24",
   "metadata": {},
   "source": [
    "## 2.4. Store dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0aa462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df.to_parquet('news/valid_dates.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
